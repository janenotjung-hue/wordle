{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e8ee1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from Wordle import WordleEnv\n",
    "from models.DQN import DQN\n",
    "from models.ActorCritic import Actor, Critic\n",
    "from ReplayMemory import ReplayMemory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Initialize the environment (input subset size if necessary)\n",
    "size = None\n",
    "env = WordleEnv(subset_size=size) \n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab92040",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "89a188c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 150000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "n_actions = env.action_size\n",
    "state = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(env.state_size, env.action_size).to(device)\n",
    "target_net = DQN(env.state_size, env.action_size).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Optimizer initialization\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "# Epsilon greedy action selection\n",
    "# Gradually decrease epsilon\n",
    "# If epsilon is greater than the random sample, take random action\n",
    "# Otherwise, take the action that gives the most Q value.\n",
    "def select_action(state, available_actions, action_size):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    global eps_threshold\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Create a mask tensor for previously chosen actions\n",
    "            mask = torch.full((1, action_size), -float('inf'), device=device)\n",
    "            for idx in available_actions:\n",
    "                mask[0, idx] = 0\n",
    "\n",
    "            # Add the mask to the DQN output and select the maximum value\n",
    "            masked_output = policy_net(state) + mask\n",
    "            return masked_output.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.choice(available_actions)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a412a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb479d75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 300000\n",
    "average_reward = 0\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            average_reward += reward/1000\n",
    "            if(episode % 1000 == 0):\n",
    "                print(f\"Episode: {episode}/{num_episodes}, Attempts: {env.attempts}, Reward: {reward[0]}\")\n",
    "                average_reward = 0\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "332ea6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials: 1000, Success rate: 0.66, Average number of attempts: 5.09\n"
     ]
    }
   ],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1bceb60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials with SALET start: 1000, Success rate: 0.71, Average number of attempts: 4.95\n"
     ]
    }
   ],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    action = torch.tensor([[345]], device=device, dtype=torch.long) #Salet start.\n",
    "    for t in count():\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials with SALET start: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "32723e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SLOSH\n",
      "Target word: THORN\n",
      "Attempts left: 5\n",
      "Current guess: BOOZY\n",
      "Target word: THORN\n",
      "Attempts left: 4\n",
      "Current guess: ATOLL\n",
      "Target word: THORN\n",
      "Attempts left: 3\n",
      "Current guess: ERODE\n",
      "Target word: THORN\n",
      "Attempts left: 2\n",
      "Current guess: THORN\n",
      "Target word: THORN\n",
      "Attempts left: 2\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c73c7b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SALET\n",
      "Target word: FRONT\n",
      "Attempts left: 5\n",
      "Current guess: POINT\n",
      "Target word: FRONT\n",
      "Attempts left: 4\n",
      "Current guess: CHANT\n",
      "Target word: FRONT\n",
      "Attempts left: 3\n",
      "Current guess: FRONT\n",
      "Target word: FRONT\n",
      "Attempts left: 3\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'FRONT'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "78feac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SALET\n",
      "Target word: CHOKE\n",
      "Attempts left: 5\n",
      "Current guess: KEBAB\n",
      "Target word: CHOKE\n",
      "Attempts left: 4\n",
      "Current guess: TRAWL\n",
      "Target word: CHOKE\n",
      "Attempts left: 3\n",
      "Current guess: ACUTE\n",
      "Target word: CHOKE\n",
      "Attempts left: 2\n",
      "Current guess: DODGE\n",
      "Target word: CHOKE\n",
      "Attempts left: 1\n",
      "Current guess: ELOPE\n",
      "Target word: CHOKE\n",
      "Attempts left: 0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'CHOKE'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980ab1e",
   "metadata": {},
   "source": [
    "## Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(env.state_size, env.action_size).to(device)\n",
    "critic = Critic(env.state_size, env.action_size).to(device)\n",
    "\n",
    "def select_action_actor(state, available_actions, action_size):\n",
    "    with torch.no_grad():\n",
    "        # Create a mask tensor for previously chosen actions\n",
    "        mask = torch.full((1, action_size), -float('inf'), device=device)\n",
    "        for idx in available_actions:\n",
    "            mask[0, idx] = 0\n",
    "\n",
    "        # Add the mask to the actor output and sample from the distribution\n",
    "        actor_output = actor(state).probs\n",
    "        masked_output = actor_output + mask\n",
    "        masked_distribution = Categorical(logits=masked_output)\n",
    "        return masked_distribution.sample().view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ffd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = 0\n",
    "num_episodes = 10000\n",
    "\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=0.001)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=0.001)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "\n",
    "    for i in count():\n",
    "        dist, value = actor(state), critic(state)\n",
    "\n",
    "        action = select_action_actor(state, env.available_actions, env.action_size)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        log_prob = dist.log_prob(action).unsqueeze(1)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        if value.dim() > 1:\n",
    "            values.append(value.squeeze(0))\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            average_reward += reward/1000\n",
    "            if(episode % 1000 == 0):\n",
    "                print(f\"Episode: {episode}/{num_episodes}, Attempts: {env.attempts}, Reward: {reward}\")\n",
    "                average_reward = 0\n",
    "            break\n",
    "\n",
    "    next_value = critic(next_state)\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    returns = torch.tensor(rewards).sum()\n",
    "    if values:\n",
    "        values = torch.cat(values, dim=0).unsqueeze(1)\n",
    "    else:\n",
    "        # If values is empty, initialize it with a dummy tensor to avoid errors\n",
    "        values = torch.zeros(1, 1, device=device, requires_grad=True)\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    optimizer_actor.zero_grad()\n",
    "    optimizer_critic.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    critic_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    optimizer_critic.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action_actor(state, env.available_actions, env.action_size)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    action = torch.tensor([[345]], device=device, dtype=torch.long) #Salet start.\n",
    "    for t in count():\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "        action = select_action_actor(state, env.available_actions, env.action_size)\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials with SALET start: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5839904",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'OTHER'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action_actor(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
